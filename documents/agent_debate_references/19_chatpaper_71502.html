<!DOCTYPE html><html  lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=0, viewport-fit=cover"><title>MAD-Sherlock: Multi-Agent Debates for Out-of-Context Misinformation Detection</title><script src="https://openpanel.dev/op1.js" defer async></script><link rel="stylesheet" href="https://cdn.pdppt.com/chatpaper/_nuxt/style.C8UmFHTW.css" crossorigin><link href="https://www.clarity.ms/tag/miya3qy90m" rel="preload" crossorigin="anonymous" referrerpolicy="no-referrer" fetchpriority="low" as="script"><link rel="modulepreload" as="script" crossorigin href="https://cdn.pdppt.com/chatpaper/_nuxt/MmQ9F6jo.js"><link rel="modulepreload" as="script" crossorigin href="https://cdn.pdppt.com/chatpaper/_nuxt/XZTJCa8x.js"><link rel="modulepreload" as="script" crossorigin href="https://cdn.pdppt.com/chatpaper/_nuxt/DyAX2DkC.js"><link rel="modulepreload" as="script" crossorigin href="https://cdn.pdppt.com/chatpaper/_nuxt/bUePjVg8.js"><link rel="modulepreload" as="script" crossorigin href="https://cdn.pdppt.com/chatpaper/_nuxt/DjiwFUkp.js"><link rel="modulepreload" as="script" crossorigin href="https://cdn.pdppt.com/chatpaper/_nuxt/CKpytZCs.js"><link rel="modulepreload" as="script" crossorigin href="https://cdn.pdppt.com/chatpaper/_nuxt/Cxb5iPOI.js"><link rel="modulepreload" as="script" crossorigin href="https://cdn.pdppt.com/chatpaper/_nuxt/DXKFEiGB.js"><link rel="modulepreload" as="script" crossorigin href="https://cdn.pdppt.com/chatpaper/_nuxt/C687CptH.js"><link rel="modulepreload" as="script" crossorigin href="https://cdn.pdppt.com/chatpaper/_nuxt/B-omsRtA.js"><link rel="modulepreload" as="script" crossorigin href="https://cdn.pdppt.com/chatpaper/_nuxt/f2LxKUHZ.js"><link rel="modulepreload" as="script" crossorigin href="https://cdn.pdppt.com/chatpaper/_nuxt/Dbrw4Urc.js"><link rel="modulepreload" as="script" crossorigin href="https://cdn.pdppt.com/chatpaper/_nuxt/BkrSrWmL.js"><link rel="preload" as="fetch" fetchpriority="low" crossorigin="anonymous" href="https://cdn.pdppt.com/chatpaper/_nuxt/builds/meta/9afa1f40-0943-4a2f-9a2d-dfcc61e83be5.json"><link rel="prefetch" as="image" type="image/png" href="https://cdn.pdppt.com/chatpaper/_nuxt/error.tvJ2pCPV.png"><link rel="prefetch" as="image" type="image/png" href="https://cdn.pdppt.com/chatpaper/_nuxt/loading.R5XN8ijB.png"><link rel="prefetch" as="image" type="image/png" href="https://cdn.pdppt.com/chatpaper/_nuxt/banner-bg.B25YyVX9.png"><link rel="prefetch" as="image" type="image/png" href="https://cdn.pdppt.com/chatpaper/_nuxt/feature-1.C-wTMKJL.png"><link rel="prefetch" as="image" type="image/png" href="https://cdn.pdppt.com/chatpaper/_nuxt/feature-2.DcW65lKk.png"><link rel="prefetch" as="image" type="image/png" href="https://cdn.pdppt.com/chatpaper/_nuxt/feature-3.BcKdOcS4.png"><link rel="prefetch" as="image" type="image/png" href="https://cdn.pdppt.com/chatpaper/_nuxt/feature-4.Bq7Yme__.png"><link rel="prefetch" as="image" type="image/png" href="https://cdn.pdppt.com/chatpaper/_nuxt/loading-dark.CPdTkWpg.png"><link rel="prefetch" as="image" type="image/png" href="https://cdn.pdppt.com/chatpaper/_nuxt/banner-bg-dark.DaYh7taf.png"><link rel="prefetch" as="image" type="image/png" href="https://cdn.pdppt.com/chatpaper/_nuxt/feature-1-dark.DEGdtMvi.png"><link rel="prefetch" as="image" type="image/png" href="https://cdn.pdppt.com/chatpaper/_nuxt/feature-2-dark.CKf1DKWz.png"><link rel="prefetch" as="image" type="image/png" href="https://cdn.pdppt.com/chatpaper/_nuxt/feature-3-dark.DqukspRc.png"><link rel="prefetch" as="image" type="image/png" href="https://cdn.pdppt.com/chatpaper/_nuxt/feature-4-dark.DyYmE5rl.png"><link rel="prefetch" as="script" crossorigin href="https://cdn.pdppt.com/chatpaper/_nuxt/CauVrkL1.js"><link rel="prefetch" as="script" crossorigin href="https://cdn.pdppt.com/chatpaper/_nuxt/i96o44gQ.js"><link rel="prefetch" as="script" crossorigin href="https://cdn.pdppt.com/chatpaper/_nuxt/BAIoDDil.js"><link rel="prefetch" as="script" crossorigin href="https://cdn.pdppt.com/chatpaper/_nuxt/BiKFYMmG.js"><link rel="prefetch" as="image" type="image/svg+xml" href="https://cdn.pdppt.com/chatpaper/_nuxt/product-hunt-dark-top.Dc_g4FJb.svg"><link rel="prefetch" as="video" href="https://cdn.pdppt.com/chatpaper/_nuxt/introduce.CCmReCZW.mp4"><link rel="prefetch" as="image" type="image/png" href="https://cdn.pdppt.com/chatpaper/_nuxt/footer-bg.DpaYev1P.png"><link rel="prefetch" as="image" type="image/png" href="https://cdn.pdppt.com/chatpaper/_nuxt/chatdoc.BPozj6zK.png"><link rel="prefetch" as="image" type="image/gif" href="https://cdn.pdppt.com/chatpaper/_nuxt/chatdoc-loading.BSerDD4s.gif"><link rel="prefetch" as="image" type="image/png" href="https://cdn.pdppt.com/chatpaper/_nuxt/logo.CYVlVwPE.png"><link rel="prefetch" as="image" type="image/png" href="https://cdn.pdppt.com/chatpaper/_nuxt/chatpaper.B67XpuJ1.png"><link rel="prefetch" as="script" crossorigin href="https://cdn.pdppt.com/chatpaper/_nuxt/D0xX-XDb.js"><link rel="prefetch" as="image" type="image/png" href="https://cdn.pdppt.com/chatpaper/_nuxt/empty.trqJssam.png"><link rel="prefetch" as="image" type="image/png" href="https://cdn.pdppt.com/chatpaper/_nuxt/collection-empty-light.C77iICby.png"><link rel="prefetch" as="image" type="image/png" href="https://cdn.pdppt.com/chatpaper/_nuxt/collection-empty-dark.BhGd3W3E.png"><link rel="prefetch" as="image" type="image/png" href="https://cdn.pdppt.com/chatpaper/_nuxt/search-empty-light.BVbdWlZ9.png"><link rel="prefetch" as="image" type="image/png" href="https://cdn.pdppt.com/chatpaper/_nuxt/search-empty-dark.DMXrkyLj.png"><meta name="format-detection" content="telephone=no,email=no,address=no"><link rel="icon" type="image/x-icon" href="/favicon.png"><script type="text/javascript">window.op=window.op||function(...args){(window.op.q=window.op.q||[]).push(args);};window.op('init',{clientId:'8be32dae-1404-49c8-8c90-06ef00c8ed2d',trackScreenViews:true,trackOutgoingLinks:true,trackAttributes:true});</script><link hid="i18n-xd" rel="alternate" href="http://chatpaper.com/paper/71502" hreflang="x-default"><link hid="i18n-alt-en" rel="alternate" href="http://chatpaper.com/paper/71502" hreflang="en"><link hid="i18n-alt-zh" rel="alternate" href="http://chatpaper.com/zh-CN/paper/71502" hreflang="zh"><link hid="i18n-alt-zh-CN" rel="alternate" href="http://chatpaper.com/zh-CN/paper/71502" hreflang="zh-CN"><link hid="i18n-alt-fr" rel="alternate" href="http://chatpaper.com/fr/paper/71502" hreflang="fr"><link hid="i18n-alt-de" rel="alternate" href="http://chatpaper.com/de/paper/71502" hreflang="de"><link hid="i18n-alt-es" rel="alternate" href="http://chatpaper.com/es/paper/71502" hreflang="es"><link hid="i18n-alt-ja" rel="alternate" href="http://chatpaper.com/ja/paper/71502" hreflang="ja"><link hid="i18n-alt-pt" rel="alternate" href="http://chatpaper.com/pt/paper/71502" hreflang="pt"><link hid="i18n-can" rel="canonical" href="http://chatpaper.com/paper/71502"><meta hid="i18n-og" property="og:locale" content="en"><meta hid="i18n-og-alt-zh-CN" property="og:locale:alternate" content="zh_CN"><meta hid="i18n-og-alt-fr" property="og:locale:alternate" content="fr"><meta hid="i18n-og-alt-de" property="og:locale:alternate" content="de"><meta hid="i18n-og-alt-es" property="og:locale:alternate" content="es"><meta hid="i18n-og-alt-ja" property="og:locale:alternate" content="ja"><meta hid="i18n-og-alt-pt" property="og:locale:alternate" content="pt"><meta name="keywords" content="Dailyarxiv,dailypaper,paper resarch,latest paper,Academic Research,arxiv research,summarization tool,ai paper tool,chat paper,chatpaper,chatdoc,chatpdf,pdfai,openai,ai summary,gpt-academic,chatgpt,Artificial intelligence,Computation and Language,Computer Vision and PatternRecognition,Machine Learning"><meta property="og:site_name" content="ChatPaper"><meta property="description" content="MAD-SHERLOCK is a novel multi-agent debate framework that enhances out-of-context misinformation detection and explainability by leveraging external information retrieval and advanced reasoning capabilities, outperforming existing models without requiring extensive fine-tuning."><meta property="og:title" content="MAD-Sherlock: Multi-Agent Debates for Out-of-Context Misinformation Detection"><meta property="og:description" content="MAD-SHERLOCK is a novel multi-agent debate framework that enhances out-of-context misinformation detection and explainability by leveraging external information retrieval and advanced reasoning capabilities, outperforming existing models without requiring extensive fine-tuning."><meta name="twitter:title" content="MAD-Sherlock: Multi-Agent Debates for Out-of-Context Misinformation Detection"><meta name="twitter:description" content="MAD-SHERLOCK is a novel multi-agent debate framework that enhances out-of-context misinformation detection and explainability by leveraging external information retrieval and advanced reasoning capabilities, outperforming existing models without requiring extensive fine-tuning."><meta property="og:url" content="http://chatpaper.com/paper/71502"><meta property="og:type" content="website"><meta property="og:image" content="https://chatdoc-arxiv.oss-us-west-1.aliyuncs.com/images/arxiv/2410.20140/two_page_thumbnail.jpeg"><meta property="twitter:image" content="https://chatdoc-arxiv.oss-us-west-1.aliyuncs.com/images/arxiv/2410.20140/two_page_thumbnail.jpeg"><meta property="twitter:card" content="summary_large_image"><script type="module" src="https://cdn.pdppt.com/chatpaper/_nuxt/MmQ9F6jo.js" crossorigin></script><script>"use strict";(()=>{const t=window,e=document.documentElement,c=["dark","light"],n=getStorageValue("localStorage","chatpaper-color-mode")||"system";let i=n==="system"?u():n;const r=e.getAttribute("data-color-mode-forced");r&&(i=r),l(i),t["__NUXT_COLOR_MODE__"]={preference:n,value:i,getColorScheme:u,addColorScheme:l,removeColorScheme:d};function l(o){const s=""+o+"",a="";e.classList?e.classList.add(s):e.className+=" "+s,a&&e.setAttribute("data-"+a,o)}function d(o){const s=""+o+"",a="";e.classList?e.classList.remove(s):e.className=e.className.replace(new RegExp(s,"g"),""),a&&e.removeAttribute("data-"+a)}function f(o){return t.matchMedia("(prefers-color-scheme"+o+")")}function u(){if(t.matchMedia&&f("").media!=="not all"){for(const o of c)if(f(":"+o).matches)return o}return"light"}})();function getStorageValue(t,e){switch(t){case"localStorage":return window.localStorage.getItem(e);case"sessionStorage":return window.sessionStorage.getItem(e);case"cookie":return getCookie(e);default:return null}}function getCookie(t){const c=("; "+window.document.cookie).split("; "+t+"=");if(c.length===2)return c.pop()?.split(";").shift()}</script></head><body><div id="__nuxt"><!--[--><div class="nuxt-loading-indicator" style="position:fixed;top:0;right:0;left:0;pointer-events:none;width:auto;height:3px;opacity:0;background:var(--el-color-primary);background-size:Infinity% auto;transform:scaleX(0%);transform-origin:left;transition:transform 0.1s, height 0.4s, opacity 0.4s;z-index:999999;"></div><!--[--><header class="chatpaper-header subpath-paper-id-header has-header default-layout-header has-border" data-v-71bb4d32 data-v-5422ff3b><div class="container header" data-v-5422ff3b><div class="left" data-v-5422ff3b><a href="/" class="logo" data-v-5422ff3b data-v-8ef2e9a7><img src="https://cdn.pdppt.com/chatpaper/_nuxt/logo.CYVlVwPE.png" alt="chatpaper" width="36" data-v-8ef2e9a7><h1 data-v-8ef2e9a7>ChatPaper</h1></a><a href="/" class="go-back" data-v-5422ff3b data-v-defcbb8a><svg aria-hidden="false" style="width:32px;height:32px;--color:var(--el-text-color-secondary);--408ac560:0 5px;" class="svg-icon-base" data-v-defcbb8a><use href="#icon-arrow-left"></use></svg></a></div><div style="" class="center" data-v-5422ff3b><span data-v-5422ff3b data-v-978be2cf></span></div><div class="right" data-v-5422ff3b><div style="" class="mobile-search-view-wrapper" data-v-5422ff3b><span data-v-5422ff3b data-v-1d8e258b></span></div><span data-v-5422ff3b data-v-dfdc6b90></span><div class="el-divider el-divider--vertical" style="--el-border-style:solid;" role="separator" data-v-5422ff3b><!--v-if--></div><a class="el-link el-link--primary" data-v-5422ff3b><!--v-if--><span class="el-link__inner"><!--[-->Sign in<!--]--></span><!--v-if--></a><!----></div></div><div class="container nav" data-v-5422ff3b><div class="nav" data-v-5422ff3b data-v-7f8d44a3><ul role="menubar" style="--el-menu-level:0;" class="el-menu el-menu--horizontal" data-v-7f8d44a3><!--[--><li class="el-menu-item menu-item" role="menuitem" tabindex="-1" data-v-7f8d44a3><!--[--><!--[--><a href="/interests" class="item-link" data-v-7f8d44a3>Interests</a><!--]--><!--[--><!--]--><!--]--></li><li class="el-menu-item is-active menu-item" role="menuitem" tabindex="-1" data-v-7f8d44a3><!--[--><!--[--><a href="/" class="item-link" data-v-7f8d44a3>arXiv</a><!--]--><!--[--><!--]--><!--]--></li><li class="el-menu-item menu-item" role="menuitem" tabindex="-1" data-v-7f8d44a3><!--[--><!--[--><a href="/venues" class="item-link" data-v-7f8d44a3>Venues</a><!--]--><!--[--><!--]--><!--]--></li><li class="el-menu-item menu-item special-menu-item" role="menuitem" tabindex="-1" data-v-7f8d44a3><!--[--><!--[--><span class="item-link" data-v-7f8d44a3>Collection</span><!--]--><!--[--><!--]--><!--]--></li><!--]--></ul><span data-v-7f8d44a3></span></div><nav class="mobile-nav" data-v-5422ff3b data-v-79b1a9b2><!--[--><a href="/interests" class="nav-item" data-v-79b1a9b2><span data-v-79b1a9b2>Interests</span><!----></a><a href="/" class="nav-item active" data-v-79b1a9b2><span data-v-79b1a9b2>arXiv</span><div class="icon-wrapper" data-v-79b1a9b2><svg aria-hidden="false" style="width:20px;height:20px;--color:var(--el-color-primary);--408ac560:0;" class="svg-icon-base" data-v-79b1a9b2><use href="#icon-arrow-down"></use></svg></div></a><a href="/venues" class="nav-item" data-v-79b1a9b2><span data-v-79b1a9b2>Venues</span><!----></a><!--]--></nav></div></header><div class="banner-container" style="display:none;" data-v-71bb4d32 data-v-735d00aa><div class="banner-wrapper" data-v-735d00aa><div class="banner" data-v-735d00aa><h2 class="title" data-v-735d00aa>AI-Powered Library for Researchers</h2><h3 class="desc" data-v-735d00aa><!--[--><span class="keyword" data-v-735d00aa>Track</span><!--]--> research interests, scroll daily paper feeds with <!--[--><span class="keyword" data-v-735d00aa>AI summary</span><!--]-->, 
 and <!--[--><span class="keyword" data-v-735d00aa>chat</span><!--]--> with bulk of files.</h3><span data-v-735d00aa></span><div data-v-735d00aa><span data-v-735d00aa data-v-978be2cf></span><span data-v-735d00aa data-v-1d8e258b></span></div></div></div></div><div class="subpath-paper-id simple layout-wrapper" data-v-71bb4d32><!----><div class="layout-container" data-v-71bb4d32><!--[--><!--[--><div class="paper-detail" data-v-a94f7ef7><div class="main container" data-v-a94f7ef7><div class="doc-list detailed" data-v-a94f7ef7 data-v-eb890f67><div class="list-container" data-v-eb890f67><!--[--><!--[--><div class="document detailed" data-doc="71502" data-visible="false" style="--06c512c2:transparent;--54bb8b88:0;" data-v-133aa1d6><div class="main-content" data-v-133aa1d6><div class="el-affix" style="height:;width:;" data-v-133aa1d6><div class="" style=""><!--[--><div class="doc-name" data-v-133aa1d6 data-v-e1cdcd82><div class="doc-name-main" data-v-e1cdcd82><!--[--><!--[--><!--[--><a href="/" class="go-back" data-v-a94f7ef7 data-v-defcbb8a><svg aria-hidden="false" style="width:32px;height:32px;--color:var(--el-text-color-secondary);--408ac560:0 5px;" class="svg-icon-base" data-v-defcbb8a><use href="#icon-go-back"></use></svg></a><!--]--><!--]--><!--]--><span class="serial-number" data-v-e1cdcd82>1.</span><a href="https://arxiv.org/abs/2410.20140" target="_blank" rel="noopener noreferrer" class="doc-name-content" data-v-e1cdcd82>MAD-Sherlock: Multi-Agent Debates for Out-of-Context Misinformation Detection</a><!----></div><div class="doc-name-right" data-v-e1cdcd82><!----></div></div><!--]--></div></div><div class="doc-info" data-v-133aa1d6><!----><div class="doc-collect" data-v-133aa1d6 data-v-65e8560c><!--[--><a href="/?id=2&amp;auto_scroll=true" target="_blank" rel="noopener noreferrer" class="category-link" data-v-65e8560c data-v-65e8560c><span class="el-tag el-tag--primary el-tag--light common-tag" style="background-color:var(--el-bg-color-primary-light-2);" data-v-65e8560c data-v-d53d532b><span class="el-tag__content"><!--[--><!--[-->cs.AI<!--]--><!--]--></span><!--v-if--></span></a><!--]--><span class="el-tag el-tag--primary el-tag--light common-tag" style="background-color:var(--el-bg-color-primary-light-2);" data-v-65e8560c data-v-d53d532b><span class="el-tag__content"><!--[--><!--[--><span data-v-65e8560c>29 Oct 2024</span><!--]--><!--]--></span><!--v-if--></span><!----></div><div class="doc-author special" data-v-133aa1d6 data-v-04f0fb05><svg aria-hidden="false" style="width:24px;height:24px;--color:var(--el-text-color-regular);--408ac560:0 10px 0 0;" class="svg-icon-base" data-v-04f0fb05><use href="#icon-author"></use></svg><!--[--><span class="text-wrapper" data-v-04f0fb05><span data-v-04f0fb05>Kumud Lakara, Juil Sock, Christian Rupprecht, Philip Torr, John Collomosse, Christian Schroeder de Witt</span></span><!--]--></div><div class="doc-organization" data-v-133aa1d6 data-v-3267b6db><svg aria-hidden="false" style="width:24px;height:24px;--color:var(--el-text-color-regular);--408ac560:0 10px 0 0;" class="svg-icon-base" data-v-3267b6db><use href="#icon-organization"></use></svg><span class="text-wrapper" data-v-3267b6db><span data-v-3267b6db><!--[--><span class="organization" data-v-3267b6db>University of Oxford<!--[-->;<!--]--></span><span class="organization" data-v-3267b6db> British Broadcasting Corporation<!--[-->;<!--]--></span><span class="organization" data-v-3267b6db> Adobe Research<!----></span><!--]--></span></span></div><!----><div id="abstract" class="doc-abstract" data-v-133aa1d6>One of the most challenging forms of misinformation involves the out-of-context (OOC) use of images paired with misleading text, creating false narratives. Existing AI-driven detection systems lack explainability and require expensive fine-tuning. We address these issues with MAD-Sherlock: a Multi-Agent Debate system for OOC Misinformation Detection. MAD-Sherlock introduces a novel multi-agent debate framework where multimodal agents collaborate to assess contextual consistency and request external information to enhance cross-context reasoning and decision-making. Our framework enables explainable detection with state-of-the-art accuracy even without domain-specific fine-tuning. Extensive ablation studies confirm that external retrieval significantly improves detection accuracy, and user studies demonstrate that MAD-Sherlock boosts performance for both experts and non-experts. These results position MAD-Sherlock as a powerful tool for autonomous and citizen intelligence applications.</div><!----></div></div><!----><div style="--d23e7956:650px;" class="doc-summary" data-v-133aa1d6 data-v-37346210><p id="ai-summary" class="title" data-v-37346210><svg aria-hidden="false" style="width:24px;height:24px;--color:var(--el-text-color-yellow);--408ac560:0 5px;" class="svg-icon-base" data-v-37346210><use href="#icon-lamp-bulb"></use></svg><span class="text" data-v-37346210>AI Summary</span></p><div class="summary-container" data-v-37346210><div class="summary-content markdown-body part1" data-v-37346210>## Introduction and Problem Statement

The proliferation of digital misinformation, particularly through the out-of-context (OOC) use of authentic images paired with misleading text, poses a significant challenge to information integrity in online environments. Unlike manipulated or AI-generated media, OOC misinformation leverages real images stripped of their original context and repurposed to support false narratives, making detection especially difficult. This form of deception requires not only multimodal understanding but also cross-contextual reasoning to identify subtle inconsistencies between image content and accompanying text. Traditional AI-based forensic methods, which focus on detecting digital artifacts from image editing or deepfakes, are ineffective here because the image itself remains unaltered. The reliance on contextual misalignment rather than visual tampering necessitates a fundamentally different approach to detection.

Pretrained Large Multimodal Models (LMMs) offer promise due to their ability to jointly process image and text inputs and leverage vast world knowledge. However, off-the-shelf LMMs face critical limitations: they are prone to hallucination, may lack up-to-date contextual knowledge, and often fail to provide transparent, human-readable explanations for their decisions. These shortcomings reduce their reliability in high-stakes domains like news verification, where explainability is essential for building trust and enabling human validators to assess model logic. Furthermore, while fine-tuning LMMs on domain-specific data can improve performance, this approach is resource-intensive and difficult to scale across evolving events and domains.

To address these challenges, the authors introduce MAD-Sherlock: a Multi-Agent Debate system for OOC Misinformation Detection. The framework leverages multiple LMM agents that engage in structured debates to assess the contextual consistency of image-text pairs. By allowing agents to access external information retrieval and express independent opinions, MAD-Sherlock enhances reasoning, improves accuracy, and generates coherent, explainable outputs. The system operates without domain-specific fine-tuning, enabling rapid deployment across diverse contexts. The core innovation lies in framing misinformation detection as a dialectic process, where agents challenge each other’s reasoning, request additional evidence, and refine their conclusions through iterative dialogue. This approach not only improves detection performance but also produces detailed justifications that can be used to support both expert and non-expert users.



## Methodology and Debate Framework

MAD-Sherlock’s methodology centers on a multi-agent debate architecture designed to simulate collaborative reasoning among autonomous LMM agents. Each agent independently evaluates an image-text pair using internal knowledge and external context retrieved via reverse image search. The system then initiates a structured debate where agents present, defend, and refine their positions based on peer feedback and newly retrieved information. This process enables the identification of subtle contextual discrepancies that might be missed by single-agent models. The authors explore several debate configurations to determine the most effective setup, including asynchronous vs. synchronous communication, judged debates with a final arbiter, actor-skeptic dynamics, and disambiguation-driven debates.

The asynchronous debate configuration—where agents respond sequentially after reviewing prior arguments—emerges as the most effective. In this setup, agents believe they are debating a human interlocutor, which encourages more critical engagement and reduces premature consensus. This human-like interaction prompt prevents agents from passively agreeing with others, fostering deeper analysis and stronger argumentation. The judged debate variant introduces a separate agent that evaluates the debate transcript to make the final decision, but this does not outperform the direct consensus model. The actor-skeptic model assigns distinct roles—one agent proposes a classification while another critiques it—but limits collective reasoning. The disambiguation-based debate allows all agents to pose clarifying questions, which are resolved through external retrieval, enhancing contextual grounding.

Prompt engineering plays a crucial role in shaping agent behavior. Initial prompts instruct agents to analyze specific image details—such as watermarks, flags, or text—that may indicate temporal or spatial mismatches. Subsequent debate prompts require agents to explicitly justify their stance, identify flaws in opposing arguments, and resist blind agreement. These instructions promote rigorous reasoning and prevent echo-chamber effects. The external retrieval module enhances this process by providing real-world context: using the Bing Visual Search API, the system retrieves top-matching web pages where the image appears, scrapes their content, and summarizes it using Llama-13B. This summary is fed to the agents, allowing them to ground their assessments in up-to-date, factual context beyond their training data.


</div><div class="need-auth summary-content markdown-body part2" data-v-37346210></div><!----><!----></div><!----></div></div><div class="extra-wrapper default-layout-extra-wrapper" style="--06c512c2:transparent;--54bb8b88:0;" data-v-133aa1d6><!----></div><!--]--><!--]--></div></div></div></div><!--]--><!--]--></div></div><div class="feature-wrapper" style="display:none;" data-v-71bb4d32 data-v-d6f53005><div class="container" data-v-d6f53005><h1 class="title" data-v-d6f53005>Why ChatPaper</h1><ul class="feature-list" data-v-d6f53005><!--[--><li class="feature-item" data-v-d6f53005><div class="title-wrapper" data-v-d6f53005><div class="icon-wrapper" data-v-d6f53005><svg aria-hidden="false" style="width:40px;height:40px;--color:var(--el-text-color-secondary);--408ac560:0;" class="svg-icon-base" data-v-d6f53005><use href="#icon-feature-1"></use></svg></div><span class="text" data-v-d6f53005>Interest-Driven Paper Curation</span></div><p class="desc" data-v-d6f53005>Describe your interests as you like, be it keywords or sentence, you’ll get relevant papers every day via AI semantic matching.</p><div class="bg feature-1-bg" data-v-d6f53005></div></li><li class="feature-item" data-v-d6f53005><div class="title-wrapper" data-v-d6f53005><div class="icon-wrapper" data-v-d6f53005><svg aria-hidden="false" style="width:40px;height:40px;--color:var(--el-text-color-secondary);--408ac560:0;" class="svg-icon-base" data-v-d6f53005><use href="#icon-feature-2"></use></svg></div><span class="text" data-v-d6f53005>Top Conferences, One-Stop</span></div><p class="desc" data-v-d6f53005>IJCAI, ICML, CVPR, KDD...  Access papers from top AI conferences, all in one tool, for free.</p><div class="bg feature-2-bg" data-v-d6f53005></div></li><li class="feature-item" data-v-d6f53005><div class="title-wrapper" data-v-d6f53005><div class="icon-wrapper" data-v-d6f53005><svg aria-hidden="false" style="width:40px;height:40px;--color:var(--el-text-color-secondary);--408ac560:0;" class="svg-icon-base" data-v-d6f53005><use href="#icon-feature-3"></use></svg></div><span class="text" data-v-d6f53005>Paper Management Made Easy</span></div><p class="desc" data-v-d6f53005>Keep your research organized with our bookmarking feature. Build a well-structured knowledge hub at your fingertips.</p><div class="bg feature-3-bg" data-v-d6f53005></div></li><li class="feature-item" data-v-d6f53005><div class="title-wrapper" data-v-d6f53005><div class="icon-wrapper" data-v-d6f53005><svg aria-hidden="false" style="width:40px;height:40px;--color:var(--el-text-color-secondary);--408ac560:0;" class="svg-icon-base" data-v-d6f53005><use href="#icon-feature-4"></use></svg></div><span class="text" data-v-d6f53005>Chat With Any Paper</span></div><p class="desc" data-v-d6f53005>Got questions? Ask in ChatDOC with one single click. Use AI to pull data, clarify terms, and verify facts with our precise word-level tracing feature.</p><div class="bg feature-4-bg" data-v-d6f53005></div></li><!--]--></ul></div></div><div class="introduce-wrapper" style="display:none;" data-v-71bb4d32 data-v-8ee4326b><div class="container" data-v-8ee4326b><h1 class="title" data-v-8ee4326b>See how it works</h1><h2 class="desc" data-v-8ee4326b>Watch now 1 min</h2><div class="player-wrapper" data-v-8ee4326b data-v-a7286da7><div data-v-a7286da7></div></div><div class="button-wrapper" data-v-8ee4326b><button ariadisabled="false" type="button" class="el-button el-button--primary el-button--large started-btn" style="" data-v-8ee4326b><!--v-if--><span class=""><!--[-->Get Started<!--]--></span></button><!--v-if--></div></div><!--[--><footer class="footer" data-v-71bb4d32 data-v-1866aef7><div class="link-wrapper" data-v-1866aef7><!--[--><span class="link-item" data-v-1866aef7><a class="el-link el-link--default is-underline" href="/disclaim.html" target="_blank" rel="noopener noreferrer" data-v-1866aef7><!--v-if--><span class="el-link__inner"><!--[-->Disclaim<!--]--></span><!--v-if--></a><span class="dot" data-v-1866aef7>·</span></span><span class="link-item" data-v-1866aef7><a class="el-link el-link--default is-underline" href="https://chatdoc.com/privacy_policy.html" target="_blank" rel="noopener noreferrer" data-v-1866aef7><!--v-if--><span class="el-link__inner"><!--[-->Policy<!--]--></span><!--v-if--></a><span class="dot" data-v-1866aef7>·</span></span><span class="link-item" data-v-1866aef7><a class="el-link el-link--default is-underline" href="https://chatdoc.com/term_of_service.html" target="_blank" rel="noopener noreferrer" data-v-1866aef7><!--v-if--><span class="el-link__inner"><!--[-->Terms<!--]--></span><!--v-if--></a><span class="dot" data-v-1866aef7>·</span></span><span class="link-item" data-v-1866aef7><a class="el-link el-link--default is-underline" href="https://twitter.com/chatdoc_ai" target="_blank" rel="noopener noreferrer" data-v-1866aef7><!--v-if--><span class="el-link__inner"><!--[-->Twitter<!--]--></span><!--v-if--></a><span class="dot" data-v-1866aef7>·</span></span><span class="link-item" data-v-1866aef7><a class="el-link el-link--default is-underline" href="https://discord.gg/HFS8U89wXH" target="_blank" rel="noopener noreferrer" data-v-1866aef7><!--v-if--><span class="el-link__inner"><!--[-->Discord<!--]--></span><!--v-if--></a><span class="dot" data-v-1866aef7>·</span></span><span class="link-item" data-v-1866aef7><a class="el-link el-link--default is-underline" href="https://chatdoc.com/blog/" target="_blank" rel="noopener noreferrer" data-v-1866aef7><!--v-if--><span class="el-link__inner"><!--[-->Blog<!--]--></span><!--v-if--></a><span class="dot" data-v-1866aef7>·</span></span><span class="link-item" data-v-1866aef7><a class="el-link el-link--default is-underline" href="https://chatdoc.com/log/" target="_blank" rel="noopener noreferrer" data-v-1866aef7><!--v-if--><span class="el-link__inner"><!--[-->Changelog<!--]--></span><!--v-if--></a><!----></span><!--]--></div><img class="footer-bg" src="https://cdn.pdppt.com/chatpaper/_nuxt/footer-bg.DpaYev1P.png" alt="footer" data-v-1866aef7></footer><!--]--></div><span data-v-71bb4d32></span><!--]--><div style="display:none;" data-git-branch="stable" data-git-version="2ced9ae" data-git-commit-hash="2ced9aedbe14eb5b74fb4208be1b75ef6bfef508" data-update-date="2026-01-14 10:36:31"></div><!--]--></div><div id="teleports"></div><script type="application/json" data-nuxt-data="nuxt-app" data-ssr="true" id="__NUXT_DATA__">[["ShallowReactive",1],{"data":2,"state":65,"once":72,"_errors":73,"serverRendered":70,"path":75,"pinia":76},["ShallowReactive",3],{"$doyyMropf4":4},[5],{"id":6,"source_type":7,"source_id":8,"title":9,"abstract":10,"update_date":11,"status":12,"summary_status":13,"organization":14,"authors":15,"two_page_thumbnail":22,"category_list":23,"article_url":27,"pdf_url":28,"interactions":29,"summary":48,"summaryAnchorData":49},71502,"arxiv","2410.20140","MAD-Sherlock: Multi-Agent Debates for Out-of-Context Misinformation Detection","One of the most challenging forms of misinformation involves the out-of-context (OOC) use of images paired with misleading text, creating false narratives. Existing AI-driven detection systems lack explainability and require expensive fine-tuning. We address these issues with MAD-Sherlock: a Multi-Agent Debate system for OOC Misinformation Detection. MAD-Sherlock introduces a novel multi-agent debate framework where multimodal agents collaborate to assess contextual consistency and request external information to enhance cross-context reasoning and decision-making. Our framework enables explainable detection with state-of-the-art accuracy even without domain-specific fine-tuning. Extensive ablation studies confirm that external retrieval significantly improves detection accuracy, and user studies demonstrate that MAD-Sherlock boosts performance for both experts and non-experts. These results position MAD-Sherlock as a powerful tool for autonomous and citizen intelligence applications.",1730131200,"interaction_generated","parsed","University of Oxford; British Broadcasting Corporation; Adobe Research",[16,17,18,19,20,21],"Kumud Lakara","Juil Sock","Christian Rupprecht","Philip Torr","John Collomosse","Christian Schroeder de Witt","https://chatdoc-arxiv.oss-us-west-1.aliyuncs.com/images/arxiv/2410.20140/two_page_thumbnail.jpeg",[24],{"id":25,"tag":26,"source_type":7},2,"cs.AI","https://arxiv.org/abs/2410.20140","https://arxiv.org/pdf/2410.20140",[30,36,40,44],{"language":31,"question":32,"answer":33,"question_type":34,"generated_from":35},"english","Too Long; Don't Read","MAD-SHERLOCK is a novel multi-agent debate framework that enhances out-of-context misinformation detection and explainability by leveraging external information retrieval and advanced reasoning capabilities, outperforming existing models without requiring extensive fine-tuning.","tldr","gpt",{"language":31,"question":37,"answer":38,"question_type":39,"generated_from":35},"What problem does the paper attempt to solve?","The paper attempts to solve the problem of out-of-context (OOC) misinformation detection, focusing on the following aspects:\n\n1. **Task**: \n   - The primary task targeted by the paper is the detection of out-of-context misinformation, particularly the misleading use of images paired with text. This involves identifying instances where images are used in a context that misrepresents their original meaning, thereby creating false narratives.\n\n2. **Current Difficulties and Challenges**:\n   - **Complexity of OOC Contexts**: Traditional AI methods often struggle with the complexities involved in detecting OOC misinformation due to the nuanced relationship between images and accompanying text. The challenge lies in recognizing minute inconsistencies that indicate misinformation.\n   - **Lack of Explainability**: Existing AI-driven detection systems frequently lack transparency and do not provide coherent explanations for their predictions. This makes it difficult for users, both experts and non-experts, to understand the reasoning behind the detection outcomes.\n   - **Dependence on Fine-Tuning**: Many current methods require extensive fine-tuning on specialized datasets, which can be resource-intensive and limit the scalability of misinformation detection systems.\n\n3. **Motivation for the Research**:\n   - **Need for Improved Detection Methods**: With the increasing prevalence of digital misinformation, there is a pressing need for more effective detection methods that can operate at scale and provide clear explanations for their findings.\n   - **Enhancing User Understanding**: The research aims to enhance the understanding of misinformation detection for both experts and non-experts by providing coherent explanations, thereby improving trust in AI systems.\n   - **Scalability and Generalization**: The motivation also includes developing a system that can generalize across different domains without the need for extensive domain-specific fine-tuning, making it easier to adapt to various contexts and applications.","what",{"language":31,"question":41,"answer":42,"question_type":43,"generated_from":35},"What method does the paper propose?","The paper proposes the following methods within the MAD-SHERLOCK framework for out-of-context misinformation detection:\n\n1. **Multi-Agent Debate System**: \n   - **Description**: MAD-SHERLOCK employs a collaborative multi-agent debate system where multiple agents independently evaluate an image-text pair for misinformation. Each agent forms its own opinion and engages in a structured debate, allowing for the exchange of reasoning and the identification of ambiguities in each other's arguments. This setup enhances contextual reasoning and improves the overall accuracy of misinformation detection.\n\n2. **External Information Retrieval**: \n   - **Description**: The framework incorporates an external information retrieval module that utilizes the Bing Visual Search API to gather relevant web pages related to the image in question. This external context is crucial for the agents to make informed decisions about the image-text pair, as it provides additional information that may not be present in the training data of the models.\n\n3. **Prompt Engineering**: \n   - **Description**: The methodology includes carefully designed prompts that guide the agents in their reasoning process. Different prompts are used for initial opinion formation and subsequent debate rounds, ensuring that agents consider previous responses and refine their arguments. This structured prompting helps agents focus on critical details and enhances the coherence of their explanations.\n\n4. **Asynchronous Debate Configuration**: \n   - **Description**: The paper identifies an asynchronous debate setup as the most effective configuration, where agents respond sequentially rather than simultaneously. This allows agents to build on each other's arguments and critically analyze the responses, leading to a more organized and structured debate that is essential for detecting misinformation.\n\n5. **Disambiguation Queries**: \n   - **Description**: Agents are tasked with generating disambiguation queries to clarify or challenge the responses of other agents. This feature encourages deeper analysis and helps refine the agents' outputs by seeking additional information, thereby improving the accuracy of their conclusions.\n\n6. **Summarization Using LLM**: \n   - **Description**: The framework employs a large language model (Llama-13B) to summarize the textual information retrieved from web pages. This summarization focuses on the most relevant parts of the text, allowing agents to develop a more focused understanding of the external context, which is critical for making accurate assessments about misinformation.\n\n7. **User Studies for Evaluation**: \n   - **Description**: The effectiveness of MAD-SHERLOCK is evaluated through user studies that assess both the detection accuracy and the quality of explanations provided by the system. Participants from various professional backgrounds are involved to gauge the persuasiveness and clarity of the model's outputs, ensuring that the system is beneficial for both experts and non-experts.\n\nThese methods collectively enhance the performance and explainability of misinformation detection, addressing the challenges associated with out-of-context imagery in digital content.","how",{"language":31,"question":45,"answer":46,"question_type":47,"generated_from":35},"On which data was the experiment conducted?","The paper conducted experiments on the following datasets and provided specific experimental steps and results. Here’s a detailed listing:\n\n- **Dataset: NewsCLIPpings**\n  - **Description**: The NewsCLIPpings dataset is built based on the VisualNews dataset, consisting of image-caption pairs from four news agencies: BBC, USA Today, The Guardian, and The Washington Post. The dataset was created by generating out-of-context (OOC) samples by replacing an image in one image-caption pair with a semantically related image from a different image-caption pair. The Merged-Balanced version of the dataset includes balanced proportions of all retrieval strategies and positive/negative samples, with training, validation, and test sets containing 71,072, 7,024, and 7,264 samples, respectively.\n\n- **Experimental Steps**:\n  - **Setup**: All experiments were run on an 8 A40 (46GB) Nvidia GPU server.\n  - **Debate Setup**: Experiments were conducted to select the best debating configuration using the LLaVA model on a smaller subset containing 1,000 test samples from the NewsCLIPpings dataset. The experiments were run for k = 3 rounds or until the agents converged.\n  - **External Retrieval Module**: The Bing Visual Search API was utilized to run an image-based reverse search. The top k = 3 pages where the image appeared were selected, and text was scraped from them using the Newspaper3k library. The scraped text was then summarized using Llama-13B to ensure it did not exceed the model's maximum token length.\n\n- **Results**:\n  - **Performance Comparison**: The results of MAD-Sherlock were compared against existing out-of-context detection methods on the NewsCLIPpings dataset. The accuracy of various models was reported, with MAD-Sherlock achieving an accuracy of 90.17%, outperforming other models such as Sniffer (with fine-tuning) at 88.4% and CCN at 84.7%.\n\nThis structured approach highlights the dataset used, the experimental steps taken, and the results obtained in the study.","experiment","## Introduction and Problem Statement\n\nThe proliferation of digital misinformation, particularly through the out-of-context (OOC) use of authentic images paired with misleading text, poses a significant challenge to information integrity in online environments. Unlike manipulated or AI-generated media, OOC misinformation leverages real images stripped of their original context and repurposed to support false narratives, making detection especially difficult. This form of deception requires not only multimodal understanding but also cross-contextual reasoning to identify subtle inconsistencies between image content and accompanying text. Traditional AI-based forensic methods, which focus on detecting digital artifacts from image editing or deepfakes, are ineffective here because the image itself remains unaltered. The reliance on contextual misalignment rather than visual tampering necessitates a fundamentally different approach to detection.\n\nPretrained Large Multimodal Models (LMMs) offer promise due to their ability to jointly process image and text inputs and leverage vast world knowledge. However, off-the-shelf LMMs face critical limitations: they are prone to hallucination, may lack up-to-date contextual knowledge, and often fail to provide transparent, human-readable explanations for their decisions. These shortcomings reduce their reliability in high-stakes domains like news verification, where explainability is essential for building trust and enabling human validators to assess model logic. Furthermore, while fine-tuning LMMs on domain-specific data can improve performance, this approach is resource-intensive and difficult to scale across evolving events and domains.\n\nTo address these challenges, the authors introduce MAD-Sherlock: a Multi-Agent Debate system for OOC Misinformation Detection. The framework leverages multiple LMM agents that engage in structured debates to assess the contextual consistency of image-text pairs. By allowing agents to access external information retrieval and express independent opinions, MAD-Sherlock enhances reasoning, improves accuracy, and generates coherent, explainable outputs. The system operates without domain-specific fine-tuning, enabling rapid deployment across diverse contexts. The core innovation lies in framing misinformation detection as a dialectic process, where agents challenge each other’s reasoning, request additional evidence, and refine their conclusions through iterative dialogue. This approach not only improves detection performance but also produces detailed justifications that can be used to support both expert and non-expert users.\n\n\n\n## Methodology and Debate Framework\n\nMAD-Sherlock’s methodology centers on a multi-agent debate architecture designed to simulate collaborative reasoning among autonomous LMM agents. Each agent independently evaluates an image-text pair using internal knowledge and external context retrieved via reverse image search. The system then initiates a structured debate where agents present, defend, and refine their positions based on peer feedback and newly retrieved information. This process enables the identification of subtle contextual discrepancies that might be missed by single-agent models. The authors explore several debate configurations to determine the most effective setup, including asynchronous vs. synchronous communication, judged debates with a final arbiter, actor-skeptic dynamics, and disambiguation-driven debates.\n\nThe asynchronous debate configuration—where agents respond sequentially after reviewing prior arguments—emerges as the most effective. In this setup, agents believe they are debating a human interlocutor, which encourages more critical engagement and reduces premature consensus. This human-like interaction prompt prevents agents from passively agreeing with others, fostering deeper analysis and stronger argumentation. The judged debate variant introduces a separate agent that evaluates the debate transcript to make the final decision, but this does not outperform the direct consensus model. The actor-skeptic model assigns distinct roles—one agent proposes a classification while another critiques it—but limits collective reasoning. The disambiguation-based debate allows all agents to pose clarifying questions, which are resolved through external retrieval, enhancing contextual grounding.\n\nPrompt engineering plays a crucial role in shaping agent behavior. Initial prompts instruct agents to analyze specific image details—such as watermarks, flags, or text—that may indicate temporal or spatial mismatches. Subsequent debate prompts require agents to explicitly justify their stance, identify flaws in opposing arguments, and resist blind agreement. These instructions promote rigorous reasoning and prevent echo-chamber effects. The external retrieval module enhances this process by providing real-world context: using the Bing Visual Search API, the system retrieves top-matching web pages where the image appears, scrapes their content, and summarizes it using Llama-13B. This summary is fed to the agents, allowing them to ground their assessments in up-to-date, factual context beyond their training data.\n\n\n",[50,54,57,61],{"id":51,"title":52,"children":53},1,"Introduction and Problem Statement",[],{"id":25,"title":55,"children":56},"Methodology and Debate Framework",[],{"id":58,"title":59,"children":60},3,"Experimental Evaluation and Performance",[],{"id":62,"title":63,"children":64},4,"User Study and Real-World Applicability",[],["Reactive",66],{"$snuxt-i18n-meta":67,"$scolor-mode":68},{},{"preference":69,"value":69,"unknown":70,"forced":71},"system",true,false,["Set"],["ShallowReactive",74],{"$doyyMropf4":-1},"/paper/71502",{"user":77,"arxiv":81,"category":84,"google":349,"search":351},{"userInfoLoaded":70,"userInfo":78,"userInterestList":79,"systemInterestList":80},null,[],[],{"dayList":82,"currentDate":83},[],1769179012,{"arxivCategoryList":85,"venuesCategoryList":86,"categoryRelationList":87},[],[],[88,103,109,184,244,265,310],{"id":51,"parent_id":78,"level":89,"children":90},0,[91,93,95,97,100],{"id":25,"parent_id":51,"level":51,"children":92},[],{"id":58,"parent_id":51,"level":51,"children":94},[],{"id":62,"parent_id":51,"level":51,"children":96},[],{"id":98,"parent_id":51,"level":51,"children":99},5,[],{"id":101,"parent_id":51,"level":51,"children":102},91,[],{"id":104,"parent_id":78,"level":89,"children":105},6,[106],{"id":107,"parent_id":104,"level":51,"children":108},7,[],{"id":110,"parent_id":78,"level":89,"children":111},8,[112,124,136,148,160,172],{"id":113,"parent_id":110,"level":51,"children":114},9,[115,118,121],{"id":116,"parent_id":113,"level":25,"children":117},10,[],{"id":119,"parent_id":113,"level":25,"children":120},11,[],{"id":122,"parent_id":113,"level":25,"children":123},12,[],{"id":125,"parent_id":110,"level":51,"children":126},13,[127,130,133],{"id":128,"parent_id":125,"level":25,"children":129},14,[],{"id":131,"parent_id":125,"level":25,"children":132},15,[],{"id":134,"parent_id":125,"level":25,"children":135},16,[],{"id":137,"parent_id":110,"level":51,"children":138},19,[139,142,145],{"id":140,"parent_id":137,"level":25,"children":141},20,[],{"id":143,"parent_id":137,"level":25,"children":144},21,[],{"id":146,"parent_id":137,"level":25,"children":147},22,[],{"id":149,"parent_id":110,"level":51,"children":150},56,[151,154,157],{"id":152,"parent_id":149,"level":25,"children":153},57,[],{"id":155,"parent_id":149,"level":25,"children":156},58,[],{"id":158,"parent_id":149,"level":25,"children":159},59,[],{"id":161,"parent_id":110,"level":51,"children":162},64,[163,166,169],{"id":164,"parent_id":161,"level":25,"children":165},65,[],{"id":167,"parent_id":161,"level":25,"children":168},66,[],{"id":170,"parent_id":161,"level":25,"children":171},67,[],{"id":173,"parent_id":110,"level":51,"children":174},76,[175,178,181],{"id":176,"parent_id":173,"level":25,"children":177},77,[],{"id":179,"parent_id":173,"level":25,"children":180},78,[],{"id":182,"parent_id":173,"level":25,"children":183},79,[],{"id":185,"parent_id":78,"level":89,"children":186},17,[187,190,208,223],{"id":188,"parent_id":185,"level":51,"children":189},18,[],{"id":191,"parent_id":185,"level":51,"children":192},25,[193,196,199,202,205],{"id":194,"parent_id":191,"level":25,"children":195},26,[],{"id":197,"parent_id":191,"level":25,"children":198},27,[],{"id":200,"parent_id":191,"level":25,"children":201},28,[],{"id":203,"parent_id":191,"level":25,"children":204},29,[],{"id":206,"parent_id":191,"level":25,"children":207},30,[],{"id":209,"parent_id":185,"level":51,"children":210},51,[211,214,217,220],{"id":212,"parent_id":209,"level":25,"children":213},52,[],{"id":215,"parent_id":209,"level":25,"children":216},53,[],{"id":218,"parent_id":209,"level":25,"children":219},54,[],{"id":221,"parent_id":209,"level":25,"children":222},55,[],{"id":224,"parent_id":185,"level":51,"children":225},80,[226,229,232,235,238,241],{"id":227,"parent_id":224,"level":25,"children":228},81,[],{"id":230,"parent_id":224,"level":25,"children":231},82,[],{"id":233,"parent_id":224,"level":25,"children":234},83,[],{"id":236,"parent_id":224,"level":25,"children":237},84,[],{"id":239,"parent_id":224,"level":25,"children":240},85,[],{"id":242,"parent_id":224,"level":25,"children":243},86,[],{"id":245,"parent_id":78,"level":89,"children":246},23,[247,250,259,262],{"id":248,"parent_id":245,"level":51,"children":249},24,[],{"id":251,"parent_id":245,"level":51,"children":252},60,[253,256],{"id":254,"parent_id":251,"level":25,"children":255},61,[],{"id":257,"parent_id":251,"level":25,"children":258},62,[],{"id":260,"parent_id":245,"level":51,"children":261},63,[],{"id":263,"parent_id":245,"level":51,"children":264},71,[],{"id":266,"parent_id":78,"level":89,"children":267},31,[268,298],{"id":269,"parent_id":266,"level":51,"children":270},32,[271,274,277,280,283,286,289,292,295],{"id":272,"parent_id":269,"level":25,"children":273},33,[],{"id":275,"parent_id":269,"level":25,"children":276},34,[],{"id":278,"parent_id":269,"level":25,"children":279},35,[],{"id":281,"parent_id":269,"level":25,"children":282},37,[],{"id":284,"parent_id":269,"level":25,"children":285},38,[],{"id":287,"parent_id":269,"level":25,"children":288},39,[],{"id":290,"parent_id":269,"level":25,"children":291},44,[],{"id":293,"parent_id":269,"level":25,"children":294},45,[],{"id":296,"parent_id":269,"level":25,"children":297},46,[],{"id":299,"parent_id":266,"level":51,"children":300},72,[301,304,307],{"id":302,"parent_id":299,"level":25,"children":303},73,[],{"id":305,"parent_id":299,"level":25,"children":306},74,[],{"id":308,"parent_id":299,"level":25,"children":309},75,[],{"id":311,"parent_id":78,"level":89,"children":312},40,[313,322,325,328,337],{"id":314,"parent_id":311,"level":51,"children":315},41,[316,319],{"id":317,"parent_id":314,"level":25,"children":318},42,[],{"id":320,"parent_id":314,"level":25,"children":321},43,[],{"id":323,"parent_id":311,"level":51,"children":324},47,[],{"id":326,"parent_id":311,"level":51,"children":327},48,[],{"id":329,"parent_id":311,"level":51,"children":330},68,[331,334],{"id":332,"parent_id":329,"level":25,"children":333},69,[],{"id":335,"parent_id":329,"level":25,"children":336},70,[],{"id":338,"parent_id":311,"level":51,"children":339},87,[340,343,346],{"id":341,"parent_id":338,"level":25,"children":342},88,[],{"id":344,"parent_id":338,"level":25,"children":345},89,[],{"id":347,"parent_id":338,"level":25,"children":348},90,[],{"clientId":350},"",{"keywords":350,"searchType":51,"searchOption":352},"all"]</script><script>window.__NUXT__={};window.__NUXT__.config={public:{git:{branch:"stable",version:"2ced9ae",commitHash:"2ced9aedbe14eb5b74fb4208be1b75ef6bfef508",updateDate:"2026-01-14 10:36:31"},siteUrl:"https://chatpaper.com",jwtSecret:"b183a5824a12e10997b4210e64fff1f8","nuxt-scripts":{version:"",defaultScriptOptions:{trigger:"onNuxtReady"}},piniaPluginPersistedstate:{},i18n:{baseUrl:"",defaultLocale:"en-US",defaultDirection:"ltr",strategy:"prefix_and_default",lazy:true,rootRedirect:"",routesNameSeparator:"___",defaultLocaleRouteNameSuffix:"default",skipSettingLocaleOnNavigate:false,differentDomains:false,trailingSlash:false,locales:[{code:"en-US",language:"en",files:[{path:"/opt/build/i18n/locales/en-US.js",cache:""}]},{code:"zh-CN",language:"zh-CN",files:[{path:"/opt/build/i18n/locales/en-US.js",cache:""}]},{code:"fr",language:"fr",files:[{path:"/opt/build/i18n/locales/en-US.js",cache:""}]},{code:"de",language:"de",files:[{path:"/opt/build/i18n/locales/en-US.js",cache:""}]},{code:"es",language:"es",files:[{path:"/opt/build/i18n/locales/en-US.js",cache:""}]},{code:"ja",language:"ja",files:[{path:"/opt/build/i18n/locales/en-US.js",cache:""}]},{code:"pt",language:"pt",files:[{path:"/opt/build/i18n/locales/en-US.js",cache:""}]}],detectBrowserLanguage:{alwaysRedirect:false,cookieCrossOrigin:false,cookieDomain:"",cookieKey:"i18n_redirected",cookieSecure:false,fallbackLocale:"",redirectOn:"root",useCookie:true},experimental:{localeDetector:"",switchLocalePathLinkSSR:false,autoImportTranslationFunctions:false,typedPages:true,typedOptionsAndMessages:false,generatedLocaleFilePathFormat:"absolute",alternateLinkCanonicalQueries:false,hmr:true},multiDomainLocales:false}},app:{baseURL:"/",buildId:"9afa1f40-0943-4a2f-9a2d-dfcc61e83be5",buildAssetsDir:"/_nuxt/",cdnURL:"https://cdn.pdppt.com/chatpaper"}}</script></body></html>